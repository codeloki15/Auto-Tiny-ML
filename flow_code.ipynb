{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db83482-d178-4614-83d4-89bc5d2cd0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokeshkumar06\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\lokeshkumar06\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\chat_models\\azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://garlic-openai-api.openai.azure.com/ to https://garlic-openai-api.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lokeshkumar06\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lokeshkumar06\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\chat_models\\azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://garlic-openai-api.openai.azure.com/ to https://garlic-openai-api.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models.azure_openai import AzureChatOpenAI\n",
    "llm_config = {\n",
    "    \"deployment_name\":\"gpt35turbo16k\",\n",
    "    \"model_name\":\"gpt-35-turbo-16k\",\n",
    "    \"openai_api_key\": \"f56a89e24e7e4ce1b1369f0d70204734\",\n",
    "    \"api_version\":\"2023-06-01-preview\",\n",
    "    \"api_base\" : \"https://garlic-openai-api.openai.azure.com/\",\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "#for long context and text generation\n",
    "llm_mistral = ChatGroq(temperature=0.1, \n",
    "               groq_api_key=\"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\",\n",
    "               model_name=\"mixtral-8x7b-32768\",\n",
    "               max_tokens=30000)\n",
    "\n",
    "#for long context and text generation\n",
    "llm_mistral_cc = ChatGroq(temperature=0.7, \n",
    "               groq_api_key=\"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\",\n",
    "               model_name=\"mixtral-8x7b-32768\",\n",
    "               max_tokens=30000)\n",
    "\n",
    "#to the point answer generation\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=llm_config[\"openai_api_key\"],\n",
    "    openai_api_base=llm_config[\"api_base\"],\n",
    "    deployment_name=llm_config[\"deployment_name\"],\n",
    "    model_name=llm_config[\"model_name\"],\n",
    "    openai_api_version=llm_config[\"api_version\"],\n",
    "    temperature=llm_config[\"temperature\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59126601-d335-4b90-a6a2-817070326086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def analyze_problem(self):\n",
    "        problem_statement = input(\"Enter the problem statement here  \")\n",
    "        print('\\n\\n')\n",
    "        return problem_statement\n",
    "    \n",
    "    def train_data_location(self):\n",
    "        train_location = input(\"Enter the location of the training data CSV file: \")\n",
    "        if train_location is None:\n",
    "            print('No Test Data provided')\n",
    "        else:\n",
    "            try:\n",
    "                data = pd.read_csv(train_location)\n",
    "                print(\"Train Data loaded successfully.\")\n",
    "                print('\\n\\n')\n",
    "                return data, train_location\n",
    "            except Exception as e:\n",
    "                print(\"Error loading data:\", e)\n",
    "                return None\n",
    "\n",
    "    def test_data_location(self):\n",
    "        test_location = input(\"Enter the location of the testing data CSV file: \")\n",
    "        if test_location is None:\n",
    "            print('No Test Data provided')\n",
    "        else:\n",
    "            try:\n",
    "                test_data = pd.read_csv(test_location)\n",
    "                print(\"Test Data loaded successfully.\")\n",
    "                print('\\n\\n')\n",
    "                return test_data, test_location\n",
    "            except Exception as e:\n",
    "                print(\"Error loading data:\", e)\n",
    "                return None\n",
    "    \n",
    "    def identify_target_feature(self, data ,problem_statement):\n",
    "        features = data.columns\n",
    "        result_list = features.tolist()\n",
    "        content = llm.invoke(f\"Based on the problem statement and features provided,only provide back the target feature name. DO NOT provide anything else--> {problem_statement} and features {result_list}\")\n",
    "        target = content.content\n",
    "        if target is None:\n",
    "            target = input(\"Enter the target feature\")\n",
    "            print('\\n\\n')\n",
    "        return target\n",
    "\n",
    "\n",
    "    def propose_approach(self,data,problem_statement):\n",
    "        X = data.drop(columns=[target_feature])\n",
    "        y = data[target_feature]\n",
    "        approach = llm_mistral_cc.invoke(f'Create an exhaustive list of steps to train a model that can compete with best machine learning engineers. \\\n",
    "        The data looks like --> {data.head()}, training features are these --> {X} and target features is this -->{y}. The problem statement is--> {problem_statement}.\\\n",
    "        1. Perform Exploratory analysis, like univariate, bivariate and multi-variate analysis and create graphs to show.\\\n",
    "        2. Perform Hypothesis testing to select best features from the dataset.\\\n",
    "        3. Create new features and check the corelation and colinearity.\\\n",
    "        4. Train a model.\\\n",
    "        5. DO SHAP ANALYSIS ')\n",
    "        #print(approach)\n",
    "        return approach\n",
    "\n",
    "    def ensure_operation(self, problem_statement, target_feature, approach):\n",
    "        print(\"Ensuring operation based on:\")\n",
    "        print('\\n\\n')\n",
    "        #print(\"Problem Statement:\", problem_statement)\n",
    "        print('\\n\\n')\n",
    "        print(\"Target Feature:\", target_feature)\n",
    "        print('\\n\\n')\n",
    "        #print(\"Approach:\", approach.content)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f5ea30a-1100-430a-afc9-4abab87ff5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the problem statement here   In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the location of the training data CSV file:  C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_train_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data loaded successfully.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the location of the testing data CSV file:  C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_test_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data loaded successfully.\n",
      "\n",
      "\n",
      "\n",
      "Ensuring operation based on:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target Feature: Survived\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an Agent object\n",
    "agent = Agent(name=\"MyMLAgent\")\n",
    "\n",
    "# Analyze the problem statement\n",
    "problem_statement = agent.analyze_problem()\n",
    "\n",
    "# Load data from CSV file\n",
    "train_data, train_location = agent.train_data_location()\n",
    "test_data, test_location = agent.test_data_location()\n",
    "\n",
    "# Identify the target feature\n",
    "target_feature = agent.identify_target_feature(train_data, problem_statement)\n",
    "\n",
    "# Propose an approach\n",
    "approach = agent.propose_approach(train_data, problem_statement)\n",
    "\n",
    "# Ensure operation\n",
    "agent.ensure_operation(problem_statement, target_feature, approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065bb2d5-4f7a-4ece-8669-ba92639f28c5",
   "metadata": {},
   "source": [
    "## Example Details \n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "\n",
    "\n",
    "C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_train_data.csv\n",
    "\n",
    "C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_test_data.csv\n",
    "\n",
    "\n",
    "\n",
    "**************************************************************************************************************************\n",
    "It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n",
    "\n",
    "\n",
    "C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\train_housing_prices_data.csv\n",
    "C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\test_housing_prices_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cd2af41-352b-40fb-a45e-08beb2d0f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an exhaustive list of steps to train a model that can compete with the best machine learning engineers:\n",
      "\n",
      "1. **Data Preprocessing:**\n",
      "   - Check for missing values and handle them by imputation, deletion or using a model that can handle missing values.\n",
      "   - Check for outliers and handle them by removing or transforming the data.\n",
      "   - Check for data types and convert them into appropriate formats (e.g. categorical to numerical).\n",
      "   - Check for skewness and normalize the data if necessary.\n",
      "\n",
      "2. **Exploratory Data Analysis (EDA):**\n",
      "   - Perform univariate analysis to understand the distribution of each feature.\n",
      "   - Perform bivariate analysis to understand the relationship between two features.\n",
      "   - Perform multivariate analysis to understand the relationship between multiple features.\n",
      "   - Create visualizations like histograms, boxplots, scatterplots, heatmaps, etc. to support your analysis.\n",
      "\n",
      "3. **Feature Engineering:**\n",
      "   - Create new features that can help improve the model's performance.\n",
      "   - Check for multicollinearity and remove or combine features if necessary.\n",
      "   - Check for correlation and remove or combine features if necessary.\n",
      "   - Check for interaction effects between features.\n",
      "\n",
      "4. **Hypothesis Testing:**\n",
      "   - Perform statistical tests like Chi-Square, ANOVA, T-test, etc. to select the best features.\n",
      "   - Use p-values and confidence intervals to make data-driven decisions.\n",
      "\n",
      "5. **Model Selection:**\n",
      "   - Choose the appropriate model based on the problem statement and the data.\n",
      "   - Split the data into training and testing sets.\n",
      "   - Use cross-validation techniques like k-fold cross-validation, stratified cross-validation, etc. to avoid overfitting.\n",
      "   - Use appropriate evaluation metrics like accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
      "\n",
      "6. **Model Training:**\n",
      "   - Train the model on the training data.\n",
      "   - Tune the hyperparameters using techniques like Grid Search, Random Search, Bayesian Optimization, etc.\n",
      "   - Use regularization techniques like L1, L2 or both to prevent overfitting.\n",
      "\n",
      "7. **Model Evaluation:**\n",
      "   - Evaluate the model on the testing data.\n",
      "   - Check for underfitting or overfitting.\n",
      "   - Use techniques like Learning Curves, Confusion Matrix, ROC Curve, etc. to evaluate the model.\n",
      "\n",
      "8. **Model Interpretation:**\n",
      "   - Use techniques like SHAP (SHapley Additive exPlanations) analysis to understand the feature importance and the impact of each feature on the prediction.\n",
      "   - Use techniques like Partial Dependence Plots (PDP) or Individual Conditional Expectation (ICE) plots to understand the relationship between the features and the target variable.\n",
      "\n",
      "9. **Model Deployment:**\n",
      "   - Deploy the model in a production environment.\n",
      "   - Monitor the model's performance over time.\n",
      "   - Retrain the model with new data if necessary.\n",
      "\n",
      "10. **Continuous Improvement:**\n",
      "    - Continuously monitor the model's performance and make improvements as necessary.\n",
      "    - Keep up-to-date with the latest techniques and best practices in machine learning.\n",
      "\n",
      "By following these steps, you can train a model that can compete with the best machine learning engineers.\n"
     ]
    }
   ],
   "source": [
    "print(approach.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1258737-0172-4bb5-b930-05f6062fa40d",
   "metadata": {},
   "source": [
    "#Initialize a dictionary to store the steps\n",
    "finalized_approach = {}\n",
    "\n",
    "#Split the text by section headings\n",
    "sections = re.split(r'\\n\\d+\\. \\*\\*', approach.content)\n",
    "\n",
    "#Remove the first empty string in sections list\n",
    "sections.pop(0)\n",
    "\n",
    "#Extract steps for each section\n",
    "for section in sections:\n",
    "    # Split each section into section name and steps\n",
    "    section_parts = section.split('\\n')\n",
    "    section_name = section_parts[0]\n",
    "    steps = [step.strip() for step in section_parts[1:] if step.strip()]\n",
    "    # Store section name and steps in the dictionary\n",
    "    finalized_approach[section_name] = steps\n",
    "\n",
    "#Print the dictionary\n",
    "print(finalized_approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2a22632-7f60-46e8-aea0-e800206f17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_location = r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_train_data.csv\"\n",
    "#train_location\n",
    "#test_location\n",
    "#test_data_location = r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\Data\\kaggle_data\\titanic_test_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e90a9fe7-36eb-4d1f-a76c-db76dfd5f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "import joblib\n",
      "\n",
      "# Step 1: Data Preprocessing\n",
      "train_data = pd.read_csv(\"C:/Users/lokeshkumar06/OneDrive - Nagarro/Desktop/Lokesh/Data/kaggle_data/titanic_train_data.csv\")\n",
      "test_data = pd.read_csv(\"C:/Users/lokeshkumar06/OneDrive - Nagarro/Desktop/Lokesh/Data/kaggle_data/titanic_test_data.csv\")\n",
      "\n",
      "# Handle missing values\n",
      "imputer = SimpleImputer(strategy='mean')\n",
      "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
      "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
      "\n",
      "# Handle categorical variables\n",
      "train_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\n",
      "test_data['Sex'] = test_data['Sex'].map({'male': 0, 'female': 1})\n",
      "\n",
      "# Normalize the data\n",
      "scaler = StandardScaler()\n",
      "train_data[['Age', 'Fare']] = scaler.fit_transform(train_data[['Age', 'Fare']])\n",
      "test_data[['Age', 'Fare']] = scaler.transform(test_data[['Age', 'Fare']])\n",
      "\n",
      "# Step 2: Exploratory Data Analysis (EDA)\n",
      "# Perform EDA as per your requirements\n",
      "\n",
      "# Step 3: Feature Engineering\n",
      "# Perform feature engineering as per your requirements\n",
      "\n",
      "# Step 4: Hypothesis Testing\n",
      "# Perform hypothesis testing as per your requirements\n",
      "\n",
      "# Step 5: Model Selection\n",
      "X = train_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
      "y = train_data['Survived']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Step 6: Model Training\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Step 7: Model Evaluation\n",
      "y_pred = model.predict(X_test)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "\n",
      "# Step 8: Model Interpretation\n",
      "# Perform model interpretation as per your requirements\n",
      "\n",
      "# Step 9: Model Deployment\n",
      "joblib.dump(model, \"C:/Users/lokeshkumar06/OneDrive - Nagarro/Desktop/Lokesh/saved_model/model.pkl\")\n",
      "\n",
      "# Step 10: Continuous Improvement\n",
      "# Continuously monitor the model's performance and make improvements as necessary\n"
     ]
    }
   ],
   "source": [
    "# Loop over the dictionary and generate Python code snippets\n",
    "save_model=r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\saved_model\"    \n",
    "# Generate code with LLM here\n",
    "first_draft = llm.invoke(f'Only write  python code for the approach, DONOT write anything else -->  {approach.content}.  Train data is like--> {train_data.head()}, the target_variable is--> {target_feature}. Train data location is{train_location} and test data location is {test_location}. Save the model to this location--> {save_model}.')\n",
    "print(first_draft.content)\n",
    "#fd_code_approach = first_draft.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e50ac43-d3f9-429b-969a-22c4e09b6413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8100558659217877\n",
      "Code ran successfully!\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "def execute_code(code, count):\n",
    "    try:\n",
    "        exec(code)\n",
    "        print('Code ran successfully!')\n",
    "        return None, None, count\n",
    "    except ValueError as ve:\n",
    "        # Collect the ValueError message and stack trace\n",
    "        error_message = str(ve)\n",
    "        stack_trace = traceback.format_exc()\n",
    "        print(error_message)\n",
    "        print(stack_trace)\n",
    "        return error_message, stack_trace, count + 1\n",
    "    except Exception as e:\n",
    "        # For all other exceptions, collect the error message and stack trace\n",
    "        error_message = str(e)\n",
    "        stack_trace = traceback.format_exc()\n",
    "        print(error_message)\n",
    "        print(stack_trace)\n",
    "        return error_message, stack_trace, count + 1\n",
    "\n",
    "# Initialize error message, stack trace, and count\n",
    "error_message = \"Error\"\n",
    "stack_trace = \"Stack trace\"\n",
    "count = 0\n",
    "\n",
    "# Loop until the error is sorted or a maximum number of iterations is reached\n",
    "max_iterations = 3  # Set a maximum number of iterations to avoid infinite loops\n",
    "while (error_message or stack_trace) and count < max_iterations:\n",
    "    # Execute code and get error message, stack trace, and updated count\n",
    "    error_message, stack_trace, count = execute_code(first_draft.content, count)\n",
    "    \n",
    "    # If there's an error, get a new draft from LLM Mistral and update the code\n",
    "    if error_message or stack_trace:\n",
    "        second_draft = llm_mistral_cc.invoke(f\"There is an error in the code please re-write it carefully. Use reasoning and act on it. Original code -->{first_draft.content}, error-->{error_message} and and stack trace--> {stack_trace}\")\n",
    "        first_draft.content = second_draft.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80541a13-5626-45fc-a49d-88fa24ac8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\AE_program\\train.py\n"
     ]
    }
   ],
   "source": [
    "#Define the filename to save the content\n",
    "filename = r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\AE_program\\train.py\"\n",
    "\n",
    "#Write the content to the Python file\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(first_draft.content)\n",
    "\n",
    "print(f\"Content saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb76fb2-6607-4efb-b34c-8ccb63fe2b2b",
   "metadata": {},
   "source": [
    "## Code explanation block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f14758de-6383-4d2f-946a-70d69741e848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\code_explanation\\code_explanation.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the filename\n",
    "train_code = r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\AE_program\\train.py\"\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(train_code, \"r\") as file:\n",
    "    # Read the content of the file\n",
    "    train_content = file.read()\n",
    "code_explanation = llm_mistral_cc.invoke(f'Look into the code-->{train_content} and explain the code along with the reason in detail, explain it line by line. Take one block explain it then take another and explain it and so on. Explain it like you are explaining it to a business man who doesnt have coding experience. Explain why we did something like..why we handle missiing data and things like that.')\n",
    "# Define the file path\n",
    "file_path = r\"C:\\Users\\lokeshkumar06\\OneDrive - Nagarro\\Desktop\\Lokesh\\code_explanation\\code_explanation.txt\"\n",
    "# Write the text to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(code_explanation.content)\n",
    "print(\"Text saved to:\", file_path)\n",
    "#train_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c07d1a-12ba-4796-9e02-713ecbe9369a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
